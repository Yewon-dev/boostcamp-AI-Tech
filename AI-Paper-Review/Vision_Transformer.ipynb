{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Vision Transformer.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyMWoqgwcTJEEs3sar/aHsHm",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Yewon-dev/boostcamp-AI-Tech/blob/master/AI-Paper-Review/Vision_Transformer.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Vision Transformer (ViT)"
      ],
      "metadata": {
        "id": "J5e1BtzMSfCE"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "- Dataset : NoDataset - Just to see the Architecture.\n",
        "- Model : ViT-Base\n",
        "------\n",
        "1. 이미지를 여러 개의 패치(16x16)로 자른 후, 1차원 embedding demension(16x16x3)으로 만든다.\n",
        "2. A [CLASS] token is added at the beginning in order to get representation of the entire image.\n",
        "3. 각 패치마다 Position Embedding을 더해준다.\n",
        "4. Transformer Encoder을 12번 수행 (Base Model 기준)\n",
        "5. A linear classification head can be added on top of the final hidden state in order to classify images.\n"
      ],
      "metadata": {
        "id": "foInV-JOSe5f"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "![](https://blog.kakaocdn.net/dn/I6CZv/btq4W1uStWT/BBBI8YYnbCgfO8rKeZTK31/img.png)"
      ],
      "metadata": {
        "id": "UXWHtggqTrdz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# PyTorch implementation"
      ],
      "metadata": {
        "id": "tE2VWaj8qLkZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "참고 [YouTube](https://www.youtube.com/watch?v=ovB0ddFtzzA&t=2s)"
      ],
      "metadata": {
        "id": "_s7fOEJZ7Yny"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn"
      ],
      "metadata": {
        "id": "03DXxdt2SiUo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1. Patch Embedding"
      ],
      "metadata": {
        "id": "TUnwWTkvSe0Q"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xH8eMEE8SPye"
      },
      "outputs": [],
      "source": [
        "class PatchEmbed(nn.Module):\n",
        "    \"\"\"Split image into patches and then embed them\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    img_size : int\n",
        "        Size of the image (it is a square). - (384)\n",
        "      \n",
        "    patch_size : int\n",
        "        Size of the patch (it is a square). - (16)\n",
        "\n",
        "    in_chans : int\n",
        "        Number of input channels. - RGB (3)\n",
        "\n",
        "    embed_dim : int\n",
        "        The embedding dimension. - 16x16x3 (768)\n",
        "\n",
        "    \n",
        "    Attributes\n",
        "    ---------\n",
        "    n_patches : int\n",
        "        Number of patches inside of our image.\n",
        "\n",
        "    proj : nn.Conv2d\n",
        "        Convolutional layer that does both the splitting into patches\n",
        "        and their embedding.\n",
        "\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, img_size, patch_size, in_chans=3, embed_dim=768):\n",
        "        super().__init__()\n",
        "        self.img_size = img_size\n",
        "        self.patch_size = patch_size\n",
        "        self.n_patches = (img_size // patch_size) ** 2      ## (384 // 16) = 24\n",
        "\n",
        "        self.proj = nn.Conv2d(  ## Image to Patch\n",
        "            in_channels = in_chans,\n",
        "            out_channels = embed_dim,\n",
        "            kernel_size = patch_size,\n",
        "            stride = patch_size     ## patch_size만큼 옆으로 이동\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        \"\"\"Run forward pass.\n",
        "\n",
        "        Parameters\n",
        "        ----------\n",
        "        x : torch.Tensor\n",
        "            Shape `(n_samples, n_patches, img_size, img_size)`.\n",
        "\n",
        "        Returns\n",
        "        -------\n",
        "        torch.Tensor\n",
        "            Shape `(n_samples, n_patches, embed_dim)`.\n",
        "\n",
        "        \"\"\"\n",
        "        x = self.proj(x)   ## (n_samples, embed_dim, n_patches ** 0.5, n_patches ** 0.5)\n",
        "        x = x.flatten(2)   ## (n_samples, embed_dim, n_patches)\n",
        "        x = x.transpose(1, 2)   ## (n_samples, n_patches, embed_dim)\n",
        "\n",
        "        return x"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "- 입력 이미지 사이즈 (384, 384)\n",
        "- Convolution 수행 -> (n, 768, 24, 24)\n",
        "- flatten 과 transpose 수행 -> (n, 576, 768)"
      ],
      "metadata": {
        "id": "41B14TeV_vdX"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "-----"
      ],
      "metadata": {
        "id": "6Sy8pn0S_wGU"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2. Multi-Head Attention"
      ],
      "metadata": {
        "id": "ZViF3L9D4p7g"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Attention(nn.Module):\n",
        "    \"\"\"Attention mechanism.\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    dim : int\n",
        "        The Input and out dimension of per token features. - (input = output)\n",
        "\n",
        "    n_heads : int\n",
        "        Number of attention heads. - (12)\n",
        "\n",
        "    qkv_bias : bool\n",
        "        If True then we include bias to the query, key and value projections.\n",
        "\n",
        "    atten_p : float\n",
        "        Dropout probability applied to the query, key and value tensors.\n",
        "\n",
        "    proj_p : float\n",
        "        Dropout probability applied to the output tensor.\n",
        "        - any dropout in this model\n",
        "\n",
        "\n",
        "    Attributes\n",
        "    ----------\n",
        "    scale : float\n",
        "        Normalizing constant for the dot product.\n",
        "\n",
        "    qkv : nn.Linear\n",
        "        Linear projection for the query, key and value.\n",
        "\n",
        "    proj : nn.Linear\n",
        "        Linear mapping that takes in the concatenated output of all attention heads\n",
        "        and maps it into a new space.\n",
        "    \n",
        "    attn_drop, proj_drop : nn.Dropout\n",
        "        Dropout layers.\n",
        "\n",
        "    \"\"\"\n",
        "\n",
        "\n",
        "    def __init__(self, dim, n_heads=12, qkv_bias=True, attn_p=0., proj_p=0.):\n",
        "        super().__init__()\n",
        "        self.n_heads = n_heads\n",
        "        self.dim = dim\n",
        "        self.head_dim = dim // n_heads\n",
        "        self.scale = self.head_dim ** -0.5  ## 1 / root(self.head_dim)\n",
        "\n",
        "        self.qkv = nn.Linear(dim, dim * 3, bias = qkv_bias)\n",
        "        self.attn_drop = nn.Dropout(attn_p)\n",
        "        self.proj = nn.Linear(dim, dim)\n",
        "        self.proj_drop = nn.Dropout(proj_p)\n",
        "\n",
        "    def forward(self, x):\n",
        "        \"\"\"Run forward pass.\n",
        "\n",
        "        Parameters\n",
        "        ----------\n",
        "        x : torch.Tensor\n",
        "            Shape `(n_samples, n_patches + 1, dim)`.\n",
        "\n",
        "        Returns\n",
        "        -------\n",
        "        torch.Tensor\n",
        "            Shape `(n_samples, n_patches + 1, dim)`.\n",
        "        \"\"\"\n",
        "\n",
        "        n_samples, n_tokens, dim = x.shape\n",
        "\n",
        "        if dim != self.dim:\n",
        "            raise ValueError\n",
        "\n",
        "\n",
        "        qkv = self.qkv(x)   ## (n_samples, n_patches + 1, 3 * dim)\n",
        "        qkv = qkv.reshape(n_samples, n_tokens, 3, self.n_heads, self.head_dim)  \n",
        "                        ## (n_samples, n_patches+1, 3, n_heads, head_dim)\n",
        "        qkv = qkv.permute(2, 0, 3, 1, 4)   \n",
        "                        ## (3, n_samples, n_heads, n_patches + 1, head_dim)\n",
        "\n",
        "        ## 각각의 n_heads끼리 query, key, value로 나눔\n",
        "        q, k ,v = qkv[0], qkv[1], qkv[2]\n",
        "        k_t = k.transpose(-2, -1)       ## (n_samples, n_heads, head_dim, n_patches + 1)\n",
        "\n",
        "        ## Dot Product (Query와 Key의 유사도)\n",
        "        dp = (q @ k_t) * self.scale     ## (n_samples, n_heads, n_patches + 1, n_patches + 1)\n",
        "        attn = dp.softmax(dim=1)        ## (n_samples, n_heads, n_patches + 1, n_patches + 1)\n",
        "        attn = self.attn_drop(attn)\n",
        "\n",
        "        weighted_avg = attn @ v         ## (n_samples, n_heads, n_patches + 1, head_dim)\n",
        "\n",
        "        weighted_avg = weighted_avg.transpose(1, 2)  ## (n_samples, n_patches+1, n_heads, head_dim)\n",
        "        weighted_avg = weighted_avg.flatten(2)       ## concat (n_samples, n_patches + 1, dim)\n",
        "\n",
        "        x = self.proj(weighted_avg)     ## linear projection (n_samples, n_patches+1, dim)\n",
        "        x = self.proj_drop(x)\n",
        "        return x"
      ],
      "metadata": {
        "id": "ZFjATJSRqYSg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "`qkv = self.qkv(x)` :  Query, Key, Value로 분할하기 위해, Dimension을 3배로 키움\n",
        "\n",
        "`weighted_avg = (weighted_avg.transpose(1,2)).flatten(2)` : attention heads를 concat"
      ],
      "metadata": {
        "id": "pq6RY95WFxOS"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "-----"
      ],
      "metadata": {
        "id": "_3k2s3D1Mcsl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3. MLP (Multi Layer Perceptron)"
      ],
      "metadata": {
        "id": "PgZ6va6gMaJ_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class MLP(nn.Module):\n",
        "    \"\"\"Multi Layer Perceptron.\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    in_features : int\n",
        "        Nummber of input features.\n",
        "    \n",
        "    hidden_features : int\n",
        "        Number of nodes in the hidden layer.\n",
        "\n",
        "    out_features : int\n",
        "        Number of output features.\n",
        "\n",
        "    p : float\n",
        "        Dropout probability.\n",
        "\n",
        "    \n",
        "    Attributes\n",
        "    ----------\n",
        "    fc : nn.Linear\n",
        "        The First linear layer.\n",
        "    \n",
        "    act : nn.GELU\n",
        "        GELU activation function. - (Guassian error linear unit)\n",
        "\n",
        "    fc2 : nn.Linear\n",
        "        The second linear layer.\n",
        "\n",
        "    drop : nn.Dropout\n",
        "        Dropout Layer.\n",
        "    \"\"\"\n",
        "    def __init__(self, in_features, hidden_features, out_features, p=0.):\n",
        "        super().__init__()\n",
        "        self.fc1 = nn.Linear(in_features, hidden_features)\n",
        "        self.act = nn.GELU()\n",
        "        self.fc2 = nn.Linear(hidden_features, out_features)\n",
        "        self.drop = nn.Dropout(p)\n",
        "\n",
        "    def forward(self, x):\n",
        "        \"\"\"Run forward pass.\n",
        "\n",
        "        Parameters\n",
        "        ----------\n",
        "        x : torch.Tensor\n",
        "            Shape `(n_samples, n_patches + 1, in_features)`.\n",
        "        \n",
        "        Returns\n",
        "        -------\n",
        "        torch.Tensor\n",
        "            Shape `(n_samples, n_patches + 1, out_features)`.\n",
        "\n",
        "        \"\"\"\n",
        "        x = self.fc1(x)     ## (n_samples, n_patches + 1, hidden_features)\n",
        "        x = self.act(x)     ## (n_samples, n_patches + 1, hidden_features)\n",
        "        x = self.drop(x)    ## (n_samples, n_patches + 1, hidden_features)\n",
        "        x = self.fc2(x)     ## (n_samples, n_patches + 1, hidden_features)\n",
        "        x = self.drop(x)    ## (n_samples, n_patches + 1, hidden_features)\n",
        "\n",
        "        return x"
      ],
      "metadata": {
        "id": "MgZGpcVSMbKC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "- hidden dimension는 3072 (base model)\n",
        "- GELU : 다른 activation func보다 수렴 속도가 빠름"
      ],
      "metadata": {
        "id": "py4zt5XkQJ9h"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "-----"
      ],
      "metadata": {
        "id": "gAfvSA3vQBBF"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 4. Transformer Encoder Block"
      ],
      "metadata": {
        "id": "C2Ajt4NlQEv7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Block(nn.Module):\n",
        "    \"\"\"Transformer Block.\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    dim : int\n",
        "        Embedding dimension\n",
        "\n",
        "    n_heads : int\n",
        "        Number of attention heads.\n",
        "\n",
        "    mlp_ratio : float\n",
        "        Determines the hidden dimension size of the 'MLP' module with respect to 'dim'.\n",
        "\n",
        "    qkv_bias : bool\n",
        "        If True then we include bias to the query, key and value projections.\n",
        "\n",
        "    p, attn_p : float\n",
        "        Dropout probability\n",
        "\n",
        "\n",
        "    Attributes\n",
        "    ----------\n",
        "    norm1, norm2 : LayerNorm\n",
        "        Layer normalization.\n",
        "\n",
        "    attn : Attention\n",
        "        Attention module.\n",
        "\n",
        "    mlp : MLP\n",
        "        MLP module.    \n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, dim, n_heads, mlp_ratio=4.0, qkv_bias=True, p=0., attn_p=0.):\n",
        "        super(Block, self).__init__()\n",
        "        self.norm1 = nn.LayerNorm(dim, eps=1e-6)\n",
        "        self.attn = Attention(      ## Multi Head Attention\n",
        "            dim,\n",
        "            n_heads=n_heads,\n",
        "            qkv_bias=qkv_bias,\n",
        "            attn_p=attn_p,\n",
        "            proj_p=p\n",
        "        )\n",
        "        self.norm2 = nn.LayerNorm(dim, eps=1e-6)\n",
        "        hidden_features = int(dim * mlp_ratio)  # 3072(MLP size)\n",
        "        self.mlp = MLP(\n",
        "            in_features=dim,\n",
        "            hidden_features= hidden_features,\n",
        "            out_features=dim,       ## input features dim == output features dim\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        \"\"\"Run forward pass.\n",
        "\n",
        "        Parameters\n",
        "        ----------\n",
        "        x : torch.Tensor\n",
        "            Shape `(n_samples, n_patches + 1, dim)`.\n",
        "        \n",
        "        Returns\n",
        "        -------\n",
        "        torch.Tensor\n",
        "            Shape `(n_samples, n_patches + 1, dim)`.\n",
        "        \"\"\"\n",
        "\n",
        "        x = x + self.attn(self.norm1(x)) ## Residual block\n",
        "        x = x + self.mlp(self.norm2(x))\n",
        "        return x"
      ],
      "metadata": {
        "id": "xBW4Y-zQQCYo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "-----"
      ],
      "metadata": {
        "id": "ZyDwCo1VUisu"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 5. Vision Transformer"
      ],
      "metadata": {
        "id": "3cr92mufUj_K"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class VisionTransformer(nn.Module):\n",
        "    \"\"\"Simplified implementation of the Vision Transformer.\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    img_size : int\n",
        "        Both height and the width of the image (it is a square).\n",
        "\n",
        "    patch_size : int\n",
        "        Both height and the width of the patch (it is a square).\n",
        "\n",
        "    in_chans : int\n",
        "        Number of input channels.\n",
        "\n",
        "    n_classes : int\n",
        "        Number of classes.\n",
        "\n",
        "    emdeb_dim : int\n",
        "        Dimensionality of the token/patch embeddings.\n",
        "\n",
        "    depth : int\n",
        "        Number of blocks.\n",
        "\n",
        "    n_heads : int\n",
        "        Number of attention heads.\n",
        "\n",
        "    mlp_ratio : float\n",
        "        Determines the hidden dimension of the 'MLP' module.\n",
        "\n",
        "    qkv_bias : bool\n",
        "        If True then we include bias to the query, key and value projections.\n",
        "\n",
        "    p, attn_p : float\n",
        "        Dropout probability.\n",
        "\n",
        "\n",
        "    Attributes\n",
        "    ----------\n",
        "    patch_embed : PatchEmbed\n",
        "        Instance of 'PatchEmbed Layer'.\n",
        "\n",
        "    cls_token : nn.Parameter\n",
        "        Learnable parameter that will represent the first token in the sequence.\n",
        "        It has 'embed_dim' elements.\n",
        "    \n",
        "    pos_emb : nn.Parameter\n",
        "        Positional embedding of rhe cls token + all the patches.\n",
        "        It has '(n_patches + 1) * embed_dim' elements.\n",
        "\n",
        "    pos_drop : nn.Dropout\n",
        "        Dropout Layer.\n",
        "\n",
        "    blocks : nn.ModuleList\n",
        "        List of 'Block' modules.\n",
        "\n",
        "    norm : nn.LayerNorm\n",
        "        Layer normalization.\n",
        "    \"\"\"\n",
        "    def __init__(\n",
        "            self,\n",
        "            img_size=384,\n",
        "            patch_size=16,\n",
        "            in_chans=3,\n",
        "            n_classes=1000,\n",
        "            embed_dim=768,\n",
        "            depth=12,\n",
        "            n_heads=12,\n",
        "            mlp_ratio=4.,\n",
        "            qkv_bias=True,\n",
        "            p=0.,\n",
        "            attn_p=0.\n",
        "            ):\n",
        "        super().__init__()\n",
        "\n",
        "        self.patch_embed = PatchEmbed(img_size, patch_size, in_chans, embed_dim)\n",
        "        self.cls_token = nn.Parameter(torch.zeros(1, 1, embed_dim))\n",
        "        self.pos_embed = nn.Parameter(torch.zeros(1, 1+ self.patch_embed.n_patches, embed_dim))\n",
        "        self.pos_drop = nn.Dropout(p=p)\n",
        "\n",
        "        self.blocks = nn.ModuleList(\n",
        "            [\n",
        "                Block(\n",
        "                    dim=embed_dim,\n",
        "                    n_heads=n_heads,\n",
        "                    mlp_ratio=mlp_ratio,\n",
        "                    qkv_bias=qkv_bias,\n",
        "                    p=p,\n",
        "                    attn_p=attn_p,\n",
        "                )\n",
        "                for _ in range(depth)  # 12개의 block\n",
        "            ]\n",
        "        )\n",
        "\n",
        "        self.norm = nn.LayerNorm(embed_dim, eps=1e-6)\n",
        "        self.head = nn.Linear(embed_dim, n_classes)\n",
        "\n",
        "\n",
        "    def forward(self, x):\n",
        "        \"\"\"Run the forward pass.\n",
        "\n",
        "        Parameters\n",
        "        ----------\n",
        "        x : torch.Tensor\n",
        "            Shape `(n_samples, in_chans, img_size, img_size)`.\n",
        "\n",
        "        Returns\n",
        "        -------\n",
        "        logits : torch.Tensor\n",
        "            Logits over all the classes - `(n_samples, n_classes)`.\n",
        "        \"\"\"\n",
        "\n",
        "        n_samples = x.shape[0]\n",
        "        x = self.patch_embed(x)  # (n_samples, n_patches, embed_dim)\n",
        "\n",
        "        cls_token = self.cls_token.expand(n_samples, -1, -1)  # (n_samples, 1, embed_dim)\n",
        "        x = torch.cat((cls_token, x), dim=1)  # (n_samples, 1+n_patches, embed_dim)\n",
        "        x = x + self.pos_embed  # (n_samples, 1+n_patches, embed_dim)\n",
        "        x = self.pos_drop(x)\n",
        "\n",
        "        for block in self.blocks:\n",
        "            x = block(x)  # (n_samples, 577, 768)\n",
        "\n",
        "        x = self.norm(x)\n",
        "\n",
        "        cls_token_final = x[:, 0]  # just the CLS token\n",
        "        x = self.head(cls_token_final)\n",
        "\n",
        "        return x"
      ],
      "metadata": {
        "id": "OJCrlblcUkYn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "`x = self.patch_embed(x) ~~`\n",
        "- 처음 이미지를 (n, 576, 768)로 만든 후,\n",
        "- class token에 n_patches 차원을 더해주고 position embedding 함\n",
        "\n",
        "`ls_token_final = x[:, 0] ~~`\n",
        "- class token만 따로 추출해서 classification 수행\n",
        "- (class token이 이미지 전체의 embedding을 표현하고 있다고 가정)\n",
        "- cls_token_final의 최댓값이 예측값이 됨"
      ],
      "metadata": {
        "id": "HAeEc6zBahDj"
      }
    }
  ]
}