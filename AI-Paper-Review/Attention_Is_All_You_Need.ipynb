{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Attention-Is-All-You-Need.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyOy/SkWrOZypFMKWWtnhKvF",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Yewon-dev/boostcamp-AI-Tech/blob/master/AI-Paper-Review/Attention_Is_All_You_Need.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "[Paper](https://arxiv.org/pdf/1706.03762.pdf) / \n",
        "[Implementation guide](http://nlp.seas.harvard.edu/2018/04/03/attention.html)\n",
        "\n",
        "**Key Words**\n",
        "- Encoder-Decoder Stacks\n",
        "- Attention\n",
        "- Scaled Dot-Product Attention\n",
        "- Multi-Head Attention\n",
        "- Position-wise Feed-Forward Networks\n",
        "- Embedding and Softmax\n",
        "- Positional Encoding\n"
      ],
      "metadata": {
        "id": "se85RJZea-1N"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Abstract \n",
        "\n",
        "본 논문은 attention mechanisms에 기반을 둔 **Transformer**을 제안하고 있다. \n",
        "\n",
        "그동안 RNN, LSTM, GRU과 같은 모델들이 sequence modeling에 효과적으로 사용되었다.\n",
        "\n",
        "다만 Recurrent model들은 한번에 한 단어씩 넣는 방법처럼 sequence에 들어있는 각각의 token들의 `(1) postion 정보들을 먼저 정렬`한 후, 이것을 `(2) 반복적으로 input`에 넣고 `(3) hidden state를 update`하는 방법으로 진행된다. 이는 병렬적으로 단어를 처리하기 힘들고, 학습되는 문장의 길이가 길어지면 Long-term dependency problem이 생기게 된다.\n",
        "\n",
        "이를 보안하고자 제안된 **Attention**은 한 번의 행렬곱으로 전체의 sequence에 대한 계산이 가능하다."
      ],
      "metadata": {
        "id": "O5DwyVGZvLCV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "-----\n",
        "# Transformer Architecture\n",
        "\n",
        "<img src=\"https://lilianweng.github.io/lil-log/assets/images/transformer.png\" width=800>\n",
        "\n"
      ],
      "metadata": {
        "id": "nEkm9hhSvLFK"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "먼저 개괄적으로 Transformer의 구조를 살펴보면, Encoder-Decoder 형태로 되어 있다는 것을 알 수 있다.\n",
        "\n"
      ],
      "metadata": {
        "id": "GS1mg_NVCnU6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "-----\n",
        "## 1. Encoder-Decoder\n",
        "**Encoder** \n",
        "- \bComposed of a stack of $N$ identical layers. (문장 속 단어의 개수 = $N$)\n",
        "- 하나의 Encoder layer는 두 개의 sub-layers로 이루어져 있다. Multi-head **Self-Attention layer**과 Position wise fully connected **Feed-Forward layer**이다.\n",
        "- 각 sub-layer 이후에 **Layer-Norm**이 residual connection 형태로 수행된다.\n",
        "\n",
        "**Decoder**\n",
        "- Decoder는 3개의 sub-layer로 이루어져있다. Encoder와 동일한 2개의 sub-layers (Self-Attention, Feed-Forward)와 Encoder의 Output을 대입하여 attention을 수행하는 1개의 sub-layer (Encoder-Decoder attention)로 구성되어있다.\n",
        "- self-attention layer는 이전에 나왔던 단어들만 참고할 수 있도록 Mask를 씌운 Masked Multi-Head Attention을 사용한다.\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "UPMMs-QWDhW_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1.1 Attention\n",
        "- Attention은 **하나의 단어와 다른 위치에 있는 단어들의 관계**를 나타내는 layer로 Query, Key, Value 벡터를 이용해서 계산하는 방법이다.\n",
        "\n",
        "- `Query`는 질문을 하는 주체, `Key`는 attention을 수행하는 대상(단어)이라고 할 수 있다.\n",
        "\n",
        "Multi-Head Attention의 내부에서는 **Scale Dot-Product Attention**이라는 구조를 사용한다."
      ],
      "metadata": {
        "id": "3MNVb-YZaT6d"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1.2 Scale Dot-Product Attention\n",
        "\n",
        "<img src=\"https://production-media.paperswithcode.com/methods/SCALDE.png\" width=300>\n",
        "\n",
        "1. Query와 Key가 행렬곱을 한다.\n",
        "2. SoftMax 연산을 위해 Key의 루트값으로 나누는 값을 Scale Layer에 사용한다.\n",
        "3. Mask 벡터는 Decoder의 Masked Multi-Head Attention에서 사용한다. 이전의 sequence만 사용하기 위해 이후의 sequence들과의 관계 값을 -infinity로 설정하여 softmax의 확률을 0으로 만든다.\n",
        "4. Softmax를 이용해서 각각의 Key에 대해 Query의 가중치를 확률값으로 구한다.\n",
        "5. 앞서 구한 확률값을 Value와 행렬곱을 해서 Attention Value 값을 만든다.\n",
        "\n",
        "Attention의 함수는 다음과 같이 정의된다.\n",
        "$$Attention(Q,K,V) = softmax( \\frac{QK^T}{\\sqrt{d_k}} )V$$"
      ],
      "metadata": {
        "id": "k6HBnqAwcy3J"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1.3 Multi-Head Attention\n",
        "\n",
        "앞서 1.2의 과정들이 각각의 head마다 이루어진 후,\n",
        "concat으로 합쳐서, 기존의 Embedding 차원과 같은 차원의 output을 만들기 위해 Linear layer을 수행한다.\n",
        "\n",
        "$$ MultiHead(Q,K,V) = Concat(head_1,...,head_h)W^O$$\n",
        "$$head_i = Attention(QW^Q_i,KW^K_i,VW^V_i)$$"
      ],
      "metadata": {
        "id": "2Ei7y5Notf19"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1.4 Positional Encoding\n",
        "\n",
        "Transformer은 RNN과 다르게 단어의 위치에 대한 정보를 embedding한다. 이것을 **Positional Encoding**이라고 하는데, 본 논문에서는 sin과 cos같은 주기함수를 사용해서\b 입력을 넣었다.\n",
        "\n",
        "$$PE_{(pos,2i)} = sin(pos/10000^{2i/d_{model}})$$\n",
        "$$PE_{(pos,2i+1)} = cos(pos/10000^{2i/d_{model}})$$"
      ],
      "metadata": {
        "id": "WeaMpPmWACXc"
      }
    }
  ]
}