{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "[Basic]Optimization.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyO/JKiRicMMuRF/8tRejxKp",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Yewon-dev/boostcamp-AI-Tech/blob/master/TIL/%5BBasic%5DOptimization.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Key components\n",
        "- Generalization\n",
        "- Under-fitting vs over-fitting\n",
        "- Cross validation\n",
        "- Bias-variance tradeoff\n",
        "- Bootstrapping\n",
        "- Bagging and boosting\n"
      ],
      "metadata": {
        "id": "goDEgy3vDy8d"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Generalization\n",
        "- Generalization이 좋다 = network 성능이 학습데이터와 비슷하게 나올 것이라는 것을 보장해주는 것\n",
        "\n",
        "<img src=\"https://drive.google.com/uc?export=view&id=1NwyNW1Kbj36b0BEsHuDKWpysOW41dkpl\" width=400>"
      ],
      "metadata": {
        "id": "mbl3y74mDy3h"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Underfitting / Overfitting\n",
        "<img src=\"https://media.geeksforgeeks.org/wp-content/cdn-uploads/20190523171258/overfitting_2.png\" width = 500>"
      ],
      "metadata": {
        "id": "fMJdnvlRDyvb"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Cross-validation\n",
        "- = k fold vaildation\n",
        "- train data를 k개로 나눈 뒤, k-1개로 학습시킨 후 나머지 한개로 test(validation)한다.\n",
        "- 최적의 hyperparameters를 찾고난 후, hyperparameters를 고정시키고 모든 train data를 학습한다."
      ],
      "metadata": {
        "id": "XHJdajEeDysh"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Bias and Variance\n",
        "- High Variance : 비슷한 입력이 들어와도 출력이 달라지므로 overfitting될 가능성이 크다.\n",
        "- High bias : target에서 많이 벗어나는 것\n",
        "- train data에 noise가 껴 있을 경우, bias와 variance를 둘 다 줄이는 것은 힘들다.\n",
        "\n",
        "<img src=\"https://img1.daumcdn.net/thumb/R800x0/?scode=mtistory2&fname=https%3A%2F%2Fblog.kakaocdn.net%2Fdn%2FnZUoV%2Fbtqygi9gLOl%2FKK1oMnG6weHudeByX6Z3S0%2Fimg.png\" width=400>"
      ],
      "metadata": {
        "id": "nuRpxaj4p1G4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Bootstrapping\n",
        "- any test oe metric that uses random sampling with replacement.\n",
        "\n",
        "- **Bagging**\n",
        "  - 학습 데이터를 random subset로 만들어서 각각 모델에 학습한 후 나온 output을 값고 예측 (voting or averaging)\n",
        "  - 한 개의 모델을 쓸 때보다 좋은 성능을 낼 때가 많다.\n",
        "- **Boosting**\n",
        "  - weak learners를 sequential하게 합쳐서 하나의 strong learner를 만드는 것\n",
        "  - 한 모델에 대해 hard to classify한 데이터들을 대상으로 또 다른 모델(weak learner)을 만든다."
      ],
      "metadata": {
        "id": "4-NB74jKp1Eg"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "-----\n",
        "**Gradient Descent Methods**\n",
        "- Stochastic gradient descent\n",
        "  - single sample을 통해 update\n",
        "- Mini-batch gradient descent ✔️\n",
        "  - subset of data를 통해 update\n",
        "- Batch gradient descent\n",
        "  - whole data를 통해 update\n",
        "\n",
        "\n",
        "> large batch methods tend to converge to **sharp minimizers**\n",
        "\n",
        "> small batch methods tend to converge to **flat minimizers**\n",
        ">> sharp 보단 flat minimize가 좋다.\n",
        "\n",
        "<img src=\"https://media.vlpt.us/images/jisngprk/post/65463934-0215-4045-b93a-985ea962e930/image.png\" width=400>\n",
        "\n",
        "flat : train data에서 조금 멀어져도 generalize 잘 됨.\n"
      ],
      "metadata": {
        "id": "yrQuuZV9p1B3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Optimization\n",
        "- Stochastic gradient descent\n",
        "- Momentum\n",
        "- Nesterov accelerated gradient\n",
        "- Adagrad\n",
        "- Adadelta\n",
        "- RMSprop\n",
        "- Adam"
      ],
      "metadata": {
        "id": "UbneEyARp0_f"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## (Stochastic) gradient descent\n",
        "<img src=\"https://drive.google.com/uc?export=view&id=1QIZvTl3GC0maTU73WEC5ANFEsOEZEbbR\" width=350>"
      ],
      "metadata": {
        "id": "MTH_oY4Jp03Z"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Momentum\n",
        "<img src=\"https://drive.google.com/uc?export=view&id=1ht7USMXCwETTHFOVvbJisp-V7SJ-5moi\" width=350>"
      ],
      "metadata": {
        "id": "owhl4LdVp00y"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Nesterov Accelerated Gradient\n",
        "\n",
        "<img src=\"https://drive.google.com/uc?export=view&id=1XIhlkGIpd2gSmJVH-NSgRyjOcd6T7meS\" width=400>\n",
        "\n",
        "<img src=\"https://drive.google.com/uc?export=view&id=1HCwfARdEIrQJ31t5QTi5DweYgWHHCBQF\" width=600>"
      ],
      "metadata": {
        "id": "bS5ahVzO3E6p"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Adagrad\n",
        "- $G$ : 각 parameters가 얼마나 변했는지 저장\n",
        "- 적게 변한 parameter은 많이 변화시키고..\n",
        "\n",
        "<img src=\"https://drive.google.com/uc?export=view&id=1u2aMUCpAhyEs7NceeQmx3SuVv0Cod3fR\" width=400>"
      ],
      "metadata": {
        "id": "BcNdjfhO3E3w"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Adadelta\n",
        "- Adagrad를 확장시킨 것 (G가 무한정 커지는 것을 방지)\n",
        "- 많이 사용하지는 않음\n",
        "\n",
        "<img src=\"https://drive.google.com/uc?export=view&id=1inZyVqjOKoYvQFwy5ifKXvI1iigCAIKD\" width=400>\n"
      ],
      "metadata": {
        "id": "AxEvlR8C3E1K"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## RMSprop\n",
        "- unpublished\n",
        "\n",
        "\n",
        "<img src=\"https://drive.google.com/uc?export=view&id=1UHVoNxqOGefj9IqpjISUwZ7cvz9UC_yu\" width=400>"
      ],
      "metadata": {
        "id": "8WVcvjJ83Eyi"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Adam\n",
        "- Adaptive Moment Estimation\n",
        "- 이전의 momentum과 past gradients를 합친 것\n",
        "\n",
        "\n",
        "<img src=\"https://drive.google.com/uc?export=view&id=10j1zb8c9_VEsILb80G3fv_pRGJXKrIAW\" width=400>"
      ],
      "metadata": {
        "id": "NPK7A_E03EvY"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "------\n",
        "# Regularization\n",
        "- Early stopping\n",
        "- Parameter norm penalty\n",
        "- Data augmentation\n",
        "- Noise robustnes\n",
        "- Label smoothing\n",
        "- Dropout\n",
        "- Batch nornalization\n"
      ],
      "metadata": {
        "id": "wuRtP2uO3Ep6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Early stopping\n",
        "\n",
        "## Parameter norm penalty\n",
        "- network weight이 작으면 작을수록 좋다 -> smooth\n",
        "\n",
        "## Data augmentation\n",
        "- data가 많으면 많을수록 좋음\n",
        "- data는 한정적이므로 변형해서 새 data를 만듦\n",
        "\n",
        "## Noise robustnes\n",
        "- inputs or weights에 noise를 넣음\n",
        "\n",
        "## Label smoothing\n",
        "- train data 두개를 뽑아서 **Mix-up**\n",
        "- decision boundary를 부드럽게 만들어줌\n",
        "\n",
        "## Dropout\n",
        "- some neurons to zero\n",
        "\n",
        "## Batch nornalization\n",
        "- 학습시 평균과 분산 조정하여 정규화\n",
        "\n"
      ],
      "metadata": {
        "id": "D-LI9lPs_Ksl"
      }
    }
  ]
}